<DLPSTEXTCLASS>
<HEADER><FILEDESC><TITLESTMT><TITLE>Editor's Note [17.3]</TITLE><TITLE TYPE="sort">Editor's Note [17.3]</TITLE><AUTHOR>Maria Bonn</AUTHOR></TITLESTMT><PUBLICATIONSTMT><PUBLISHER>Michigan Publishing, University of Michigan Library</PUBLISHER><PUBPLACE>Ann Arbor, MI</PUBPLACE><IDNO TYPE="dlps">3336451.0017.300</IDNO><IDNO TYPE="purl">http://hdl.handle.net/2027/spo.3336451.0017.300</IDNO><DATE>Summer 2014</DATE><DATE TYPE="sort">2014-09-22</DATE><IDNO TYPE="doi">http://dx.doi.org/10.3998/3336451.0017.300</IDNO><AVAILABILITY TYPE="CC-BY/3.0"><P>This work is licensed under a Creative Commons Attribution 3.0 License. Please contact mpub-help@umich.edu to use this work in a way not covered by the license.</P></AVAILABILITY><AVAILABILITY><P>This work is protected by copyright and may be linked to without seeking permission. Permission must be received for subsequent distribution in print or electronically. Please contact mpub-help@umich.edu for more information.</P></AVAILABILITY></PUBLICATIONSTMT><SERIESSTMT><TITLE>Journal of Electronic Publishing</TITLE><IDNO TYPE="issn">1080-2711</IDNO><IDNO TYPE="aleph">3336451</IDNO></SERIESSTMT><SOURCEDESC><BIBL><TITLE>Editor's Note [17.3]</TITLE><AUTHOR>Maria Bonn</AUTHOR><AUTHORIND>Bonn, Maria</AUTHORIND><DATE>Summer 2014</DATE><BIBLSCOPE TYPE="volno">17</BIBLSCOPE><BIBLSCOPE TYPE="issno">3</BIBLSCOPE><BIBLSCOPE TYPE="issuetitle">Metrics for Measuring Publishing Value: Alternative and Otherwise</BIBLSCOPE></BIBL></SOURCEDESC></FILEDESC><ENCODINGDESC><EDITORIALDECL N="4"><P>This electronic text file was created via XML encoding through a series of automated and manual processes, approximating the recommendations for Level 4 of TEI Text Encoding in Libraries: Guidelines for Best Encoding Practices.</P></EDITORIALDECL></ENCODINGDESC><PROFILEDESC><TEXTCLASS><KEYWORDS></KEYWORDS></TEXTCLASS></PROFILEDESC></HEADER><TEXT>
<FRONT>
<DIV1>
<P TYPE="author">Maria Bonn</P>
<P TYPE="title">Editor&#8217;s Note [17.3]</P>
</DIV1>
</FRONT>
<BODY>
<DIV1>
<P>Historically, the value of publication has been measured by success in the marketplace and impact of the publication, whether that impact be cultural or scholarly. The calculus of this value has been as straightforward as number of copies sold (documented most widely in &#8220;best seller&#8221; lists) and/or dollars in profit generated to the complex citation and referral counts that result in a scholarly &#8220;impact factor.&#8221; Other measures, such as prize awards and reviews have also contributed to the assessment of the quality of a publication. Each of these measures has had different weight depending on the disciplinary location and publication genre of the work under discussion, as well, of course, as on the interests of the assessor (for instance a commercial publisher will measure value quite differently from a promotion and tenure review board).  As with so many areas of our cultural and intellectual lives, the widespread adoption of digital technology and networked communication (with its attendant social media practices) has disrupted our metrics of publishing value and has called for a revision of the ways in which that value is calculated. In some professional and social circles, page visits, link referrals, Google ranks, presence in the Twitter universe and other social media prominence, are now taken as seriously as scholarly citation and profit margins, a shift that raises questions for how scholars balance the emerging professional requirement for an online presence with the need for privacy and protected space for research. In addition, the value measure of page visits and glances (where a quick hit might &#8220;count&#8221; for the same as an extended period of study and engagement) are still in the early stages of development. While we have seen the rise of &#8220;altmetrics&#8221; and &#8220;impact stories,&#8221; weeks on the New York Times Best Seller List continue to indicate worthiness for attention, at least at the level of the general public (witness the international interest and continued sales success of <HI1 REND="i">Capital in the Twenty-First Century</HI1>), and the case for scholarly job security continues to be strongly influenced by citation based measures. In addition, the increased ease of collaboration and co-authoring, even across wide spans of time and space, make assigning authorial and impact &#8220;credit&#8221; both more compelling and more difficult. We are also still developing rubrics for calculating the broader social contribution of work that is made widely available via the Web. In the scholarly context this revision of measures of value continue to be embedded in disciplinary practices and prejudices, contexts that have a significant impact upon shaping evaluation metrics.</P>
<P>When <HI1 REND="i">the Journal of Electronic Publishing</HI1> invited reflections and reportage on enduring, emerging and potential measures of publication value, we expected such discussions would be rooted in the publishing context (of value to whom, for whom?) and would address both short-comings and usefulness of the metrics under discussion. While we anticipated that our contributors would be attendant to changes wrought by digital technology and networked communication, we were also interested in metrics embedded within other media cultures, both those that endure and those that are no longer current. </P>
<P>Our expectations were both overcome and overturned. The response to our call for papers for this special issue was quite strong, and we received proposals for articles addressing many forms of evaluation. The majority of articles contained herein do tilt toward &#8220;new forms,&#8221; but our authors also bring new perspectives to older forms. We are delighted to begin with a <REF TYPE="url" URL="http://dx.doi.org/10.3998/3336451.0017.301">report from the Impactstory team</REF>, a group currently leading the way in almetrics collection and reporting. They describe the current state of the art in altmetrics and its effects on publishing, and share Impactstory&#8217;s plan to build an open infrastructure for altmetrics.</P>
<P>Diesner et al. are less concerned with evaluating digital forms than they are with making use of digital technology to evaluate impact. In a <REF TYPE="url" URL="http://dx.doi.org/10.3998/3336451.0017.306">fascinating counterpoint to assessing scholarly text</REF>, they turn their attention and ours to evaluating the impact of social documentaries in film. They report on a research project where they are &#8220;developing, applying and evaluating a theoretically&#8211;grounded, empirical and computational solution for assessing the impact of social justice documentaries in a scalable, robust and rigorous fashion. [They] leverage cutting&#8211;edge methods from socio&#8211;technical data analytics&#8212;namely natural language processing and network analysis&#8212;for this purpose. [They] also built a publicly available software tool (ConText) that supports these routines.&#8221; While at least this reader could spend a long time thinking about the measures for evaluating film, questions also arise quickly about whether the methods could be applied to other formats and genres and be used across the publishing landscape.</P>
<P>Two of the articles in this issue look at less often evaluated aspects of publishing. John Duhring <REF TYPE="url" URL="http://dx.doi.org/10.3998/3336451.0017.303">undertakes a critique of the way in which would-be professionals are prepared for the market</REF>, particularly scrutinizing the movement from a &#8220;house&#8221; model of publishing to a studio model involving rapid-fire team development of digital publication (specifically, here, apps) and the value of immersing beginning publishers in that studio model.   De Grandis and Neuman <REF TYPE="url" URL="http://dx.doi.org/10.3998/3336451.0017.302">take on another kind of evaluation</REF>, the evaluation of platform models for publication. They begin with the assertion that &#8220;academic communities interested in digital publishing do not have adequate tools to help them in choosing a publishing model that suits their needs,&#8221; and go on to lay out a rubric for guiding and informing choices of models to best meet needs, with some special attention to Open Access models throughout.</P>
<P>Two articles focus upon more conventional forms of evaluation at a key moment of digital inflection. Belojevic, Sayers, and the INKE and MVP Research Teams are <REF TYPE="url" URL="http://dx.doi.org/10.3998/3336451.0017.304">engaged in developing tools to support new forms of peer review</REF>. They prototype a plugin that &#8220;could enrich the affordances of authoring and publication platforms (e.g., Open Journal Systems, WordPress, and Scalar), expand peer review, and further contextualize the practices of networked knowledge making.&#8221; They argue that the prototype&#8212;which they call &#8220;Peer Review Personas&#8221;&#8212; &#8220;enacts strategies to transform individuated feedback into peer&#8211;to&#8211;peer networks for scholarly communication.&#8221; Camilla McKay <REF TYPE="url" URL="http://dx.doi.org/10.3998/3336451.0017.305">turns to another venerable form of evaluation in a transitional moment</REF>, the book review, asserting that &#8220;given its long&#8211;standing importance, the absence of the equivalent of the book review for the world of electronic scholarship may impact its academic acceptance, especially for promotion and tenure.&#8221;</P>
<P>The perspectives and methods of our contributors to this special issue vary widely. They do all agree that evaluation is an important part of publishing, for publishers, for creators and for audiences. And they all bring exciting and engaged perspectives to the question of evaluation. I trust that our readers will find the contributors&#8217; work valuable, by many standards, and this will be demonstrated by a panoply of metrics, alternative and otherwise.</P>

</DIV1>
</BODY>
<BACK><DIV1></DIV1></BACK>
</TEXT>

</DLPSTEXTCLASS>